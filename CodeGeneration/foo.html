<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>R: Constructor and accessors for CURLOptions objects</title>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<link rel="stylesheet" type="text/css" href="../../R.css">
</head><body>

<table width="100%" summary="page for curlOptions {unknown}"><tr><td>curlOptions {unknown}</td><td align="right">R Documentation</td></tr></table>
<h2>Constructor and accessors for CURLOptions objects</h2>


<h3>Description</h3>

<p>
These functions provide a constructor
and accessor methods 
for the (currently S3) class <code>CURLOptions</code>.
This class is a way to group and manage options settings
for CURL.
These functions manage a named list of options
where the names are elements of a fixed.
Not all elements need be set, but
these functions take care of expanding names
to match the fixed set, while allowing callers
to use abbreviated/partial names.
Names that do not match (via <code><a href="../../base/html/pmatch.html">pmatch</a></code>)
will cause an error.
</p>
<p>
The set of possible names is given by
<code>names(getCurlOptionsConstants())</code>
or more directly with <code>listCurlOptions()</code>.
</p>
<p>
<code>mapCurlOptNames</code> handles the partial matching and
expansion of the names of the options for all the functions
that handle CURL options.
Currently this uses <code><a href="../../base/html/pmatch.html">pmatch</a></code> to
perform the matching and so rejects words
that are ambiguous, i.e. have multiple matches
within the set of permissible option names.
As a result, "head" will match both
"header" and "headerfunction".
We may change this behavior in the future, but
we encourage using the full names for readability of code if nothing
else.
</p>


<h3>Usage</h3>

<pre>
curlOptions(..., .opts = list())
getCurlOptionsConstants()
## S3 method for class 'CURLOptions':
x[i] &lt;- value
## S3 method for class 'CURLOptions':
x[[i]] &lt;- value
listCurlOptions()
</pre>


<h3>Arguments</h3>

<table summary="R argblock">
<tr valign="top"><td><code>...</code></td>
<td>
name-value pairs identifying the settings for the options
of interest.
<ul>
<dt>verbose</dt><dd>Set the parameter to 1 to get the library to display a lot of verbose information about its operations. Very useful for libcurl and/or protocol debugging and understanding. The verbose information will be sent to stderr, or the stream set with <code>stderr</code>.
<br>
You hardly ever want this set in production use, you will almost always want this when you debug/report problems. Another neat option for debugging is the <code>debugfunction</code>.
</dd>
<dt>header</dt><dd>A parameter set to 1 tells the library to include the header in the body output. This is only relevant for protocols that actually have headers preceding the data (like HTTP).
</dd>
<dt>noprogress</dt><dd>A parameter set to 1 tells the library to shut off the built-in progress meter completely.
<br>
Future versions of libcurl are likely to not have any built-in progress meter at all.
</dd>
<dt>nosignal</dt><dd>Pass a long. If it is 1, libcurl will not use any functions that install signal handlers or any functions that cause signals to be sent to the process. This option is mainly here to allow multi-threaded unix applications to still set/use all timeout options etc, without risking getting signals. (Added in 7.10)
<br>
If this option is set and libcurl has been built with the standard name resolver, timeouts will not occur while the name resolve takes place. Consider building libcurl with c-ares support to enable asynchronous DNS lookups, which enables nice timeouts for name resolves without signals.
</dd>
<dt>writefunction</dt><dd>Function pointer that should match the following prototype: size_t function( void *ptr, size_t size, size_t nmemb, void *stream); This function gets called by libcurl as soon as there is data received that needs to be saved. The size of the data pointed to by ptr is size multiplied with nmemb, it will not be zero terminated. Return the number of bytes actually taken care of. If that amount differs from the amount passed to your function, it'll signal an error to the library and it will abort the transfer and return CURLE_WRITE_ERROR.
<br>
From 7.18.0, the function can return CURL_WRITEFUNC_PAUSE which then will cause writing to this connection to become paused. See curl_easy_pause(3) for further details.
<br>
This function may be called with zero bytes data if the transferred file is empty.
<br>
Set this option to NULL to get the internal default function. The internal default function will write the data to the FILE * given with <code>writedata</code>.
<br>
Set the stream argument with the <code>writedata</code> option.
<br>
The callback function will be passed as much data as possible in all invokes, but you cannot possibly make any assumptions. It may be one byte, it may be thousands. The maximum amount of data that can be passed to the write callback is defined in the curl.h header file: CURL_MAX_WRITE_SIZE.
</dd>
<dt>writedata</dt><dd>Data pointer to pass to the file write function. If you use the <code>writefunction</code> option, this is the pointer you'll get as input. If you don't use a callback, you must pass a 'FILE *' as libcurl will pass this to fwrite() when writing data.
<br>
The internal <code>writefunction</code> will write the data to the FILE * given with this option, or to stdout if this option hasn't been set.
<br>
If you're using libcurl as a win32 DLL, you MUST use the <code>writefunction</code> if you set this option or you will experience crashes.
<br>
This option is also known with the older name <code>file</code>, the name <code>writedata</code> was introduced in 7.9.7.
</dd>
<dt>readfunction</dt><dd>Function pointer that should match the following prototype: size_t function( void *ptr, size_t size, size_t nmemb, void *stream); This function gets called by libcurl as soon as it needs to read data in order to send it to the peer. The data area pointed at by the pointer ptr may be filled with at most size multiplied with nmemb number of bytes. Your function must return the actual number of bytes that you stored in that memory area. Returning 0 will signal end-of-file to the library and cause it to stop the current transfer.
<br>
If you stop the current transfer by returning 0 "pre-maturely" (i.e before the server expected it, like when you've said you will upload N bytes and you upload less than N bytes), you may experience that the server "hangs" waiting for the rest of the data that won't come.
<br>
The read callback may return CURL_READFUNC_ABORT to stop the current operation immediately, resulting in a CURLE_ABORTED_BY_CALLBACK error code from the transfer (Added in 7.12.1)
<br>
From 7.18.0, the function can return CURL_READFUNC_PAUSE which then will cause reading from this connection to become paused. See curl_easy_pause(3) for further details.
<br>
If you set the callback pointer to NULL, or don't set it at all, the default internal read function will be used. It is simply doing an fread() on the FILE * stream set with <code>readdata</code>.
</dd>
<dt>readdata</dt><dd>Data pointer to pass to the file read function. If you use the <code>readfunction</code> option, this is the pointer you'll get as input. If you don't specify a read callback but instead rely on the default internal read function, this data must be a valid readable FILE *.
<br>
If you're using libcurl as a win32 DLL, you MUST use a <code>readfunction</code> if you set this option.
<br>
This option was also known by the older name <code>infile</code>, the name <code>readdata</code> was introduced in 7.9.7.
</dd>
<dt>ioctlfunction</dt><dd>Function pointer that should match the curl_ioctl_callback prototype found in &lt;curl/curl.h&gt;. This function gets called by libcurl when something special I/O-related needs to be done that the library can't do by itself. For now, rewinding the read data stream is the only action it can request. The rewinding of the read data stream may be necessary when doing a HTTP PUT or POST with a multi-pass authentication method.  (Option added in 7.12.3).
<br>
Use <code>seekfunction</code> instead to provide seeking!
</dd>
<dt>ioctldata</dt><dd>Pass a pointer that will be untouched by libcurl and passed as the 3rd argument in the ioctl callback set with <code>ioctlfunction</code>.  (Option added in 7.12.3)
</dd>
<dt>seekfunction</dt><dd>Function pointer that should match the following prototype: int function(void *instream, curl_off_t offset, int origin); This function gets called by libcurl to seek to a certain position in the input stream and can be used to fast forward a file in a resumed upload (instead of reading all uploaded bytes with the normal read function/callback). It is also called to rewind a stream when doing a HTTP PUT or POST with a multi-pass authentication method. The function shall work like "fseek" or "lseek" and accepted SEEK_SET, SEEK_CUR and SEEK_END as argument for origin, although (in 7.18.0) libcurl only passes SEEK_SET. The callback must return 0 (CURL_SEEKFUNC_OK) on success, 1 (CURL_SEEKFUNC_FAIL) to cause the upload operation to fail or 2 (CURL_SEEKFUNC_CANTSEEK) to indicate that while the seek failed, libcurl is free to work around the problem if possible. The latter can sometimes be done by instead reading from the input or similar.
<br>
If you forward the input arguments directly to "fseek" or "lseek", note that the data type for offset is not the same as defined for curl_off_t on many systems! (Option added in 7.18.0)
</dd>
<dt>seekdata</dt><dd>Data pointer to pass to the file read function. If you use the <code>seekfunction</code> option, this is the pointer you'll get as input. If you don't specify a seek callback, NULL is passed. (Option added in 7.18.0)
</dd>
<dt>sockoptfunction</dt><dd>Function pointer that should match the curl_sockopt_callback prototype found in &lt;curl/curl.h&gt;. This function gets called by libcurl after the socket() call but before the connect() call. The callback's purpose argument identifies the exact purpose for this particular socket, and currently only one value is supported: CURLSOCKTYPE_IPCXN for the primary connection (meaning the control connection in the FTP case). Future versions of libcurl may support more purposes. It passes the newly created socket descriptor so additional setsockopt() calls can be done at the user's discretion.  Return 0 (zero) from the callback on success. Return 1 from the callback function to signal an unrecoverable error to the library and it will close the socket and return CURLE_COULDNT_CONNECT.  (Option added in 7.15.6.)
</dd>
<dt>sockoptdata</dt><dd>Pass a pointer that will be untouched by libcurl and passed as the first argument in the sockopt callback set with <code>sockoptfunction</code>. (Option added in 7.15.6.)
</dd>
<dt>opensocketfunction</dt><dd>Function pointer that should match the curl_opensocket_callback prototype found in &lt;curl/curl.h&gt;. This function gets called by libcurl instead of the socket(2) call. The callback's purpose argument identifies the exact purpose for this particular socket, and currently only one value is supported: CURLSOCKTYPE_IPCXN for the primary connection (meaning the control connection in the FTP case). Future versions of libcurl may support more purposes. It passes the resolved peer address as a address argument so the callback can modify the address or refuse to connect at all. The callback function should return the socket or CURL_SOCKET_BAD in case no connection should be established or any error detected. Any additional setsockopt(2) calls can be done on the socket at the user's discretion.  CURL_SOCKET_BAD return value from the callback function will signal an unrecoverable error to the library and it will return CURLE_COULDNT_CONNECT.  This return code can be used for IP address blacklisting.  The default behavior is: 
<br>
&lt;c2&gt;&lt;a0&gt;  return socket(addr-&gt;family, addr-&gt;socktype, addr-&gt;protocol);
<br>
(Option added in 7.17.1.)
</dd>
<dt>opensocketdata</dt><dd>Pass a pointer that will be untouched by libcurl and passed as the first argument in the opensocket callback set with <code>opensocketfunction</code>. (Option added in 7.17.1.)
</dd>
<dt>progressfunction</dt><dd>Function pointer that should match the curl_progress_callback prototype found in &lt;curl/curl.h&gt;. This function gets called by libcurl instead of its internal equivalent with a frequent interval during operation (roughly once per second) no matter if data is being transfered or not.  Unknown/unused argument values passed to the callback will be set to zero (like if you only download data, the upload size will remain 0). Returning a non-zero value from this callback will cause libcurl to abort the transfer and return CURLE_ABORTED_BY_CALLBACK.
<br>
If you transfer data with the multi interface, this function will not be called during periods of idleness unless you call the appropriate libcurl function that performs transfers.
<br>
<code>noprogress</code> must be set to 0 to make this function actually get called.
</dd>
<dt>progressdata</dt><dd>Pass a pointer that will be untouched by libcurl and passed as the first argument in the progress callback set with <code>progressfunction</code>.
</dd>
<dt>headerfunction</dt><dd>Function pointer that should match the following prototype: size_t function( void *ptr, size_t size, size_t nmemb, void *stream);. This function gets called by libcurl as soon as it has received header data. The header callback will be called once for each header and only complete header lines are passed on to the callback. Parsing headers should be easy enough using this. The size of the data pointed to by ptr is size multiplied with nmemb. Do not assume that the header line is zero terminated! The pointer named stream is the one you set with the <code>writeheader</code> option. The callback function must return the number of bytes actually taken care of, or return -1 to signal error to the library (it will cause it to abort the transfer with a CURLE_WRITE_ERROR return code).
<br>
If this option is not set, or if it is set to NULL, but <code>headerdata</code> (<code>writeheader</code>) is set to anything but NULL, the function used to accept response data will be used instead. That is, it will be the function specified with <code>writefunction</code>, or if it is not specified or NULL - the default, stream-writing function.
<br>
Since 7.14.1: When a server sends a chunked encoded transfer, it may contain a trailer. That trailer is identical to a HTTP header and if such a trailer is received it is passed to the application using this callback as well. There are several ways to detect it being a trailer and not an ordinary header: 1) it comes after the response-body. 2) it comes after the final header line (CR LF) 3) a Trailer: header among the response-headers mention what header to expect in the trailer.
</dd>
<dt>writeheader</dt><dd>(This option is also known as <code>headerdata</code>) Pass a pointer to be used to write the header part of the received data to. If you don't use your own callback to take care of the writing, this must be a valid FILE *. See also the <code>headerfunction</code> option above on how to set a custom get-all-headers callback.
</dd>
<dt>debugfunction</dt><dd>Function pointer that should match the following prototype: int curl_debug_callback (CURL *, curl_infotype, string, size_t, void *); <code>debugfunction</code> replaces the standard debug function used when <code>verbose</code>  is in effect. This callback receives debug information, as specified with the curl_infotype argument. This function must return 0.  The data pointed to by the string passed to this function WILL NOT be zero terminated, but will be exactly of the size as told by the size_t argument.
<br>
Available curl_infotype values:
<br>
CURLINFO_TEXT
<br>
The data is informational text.
<br>
CURLINFO_HEADER_IN
<br>
The data is header (or header-like) data received from the peer.
<br>
CURLINFO_HEADER_OUT
<br>
The data is header (or header-like) data sent to the peer.
<br>
CURLINFO_DATA_IN
<br>
The data is protocol data received from the peer.
<br>
CURLINFO_DATA_OUT
<br>
The data is protocol data sent to the peer.
<br>
</dd>
<dt>debugdata</dt><dd>Pass a pointer to whatever you want passed in to your <code>debugfunction</code> in the last void * argument. This pointer is not used by libcurl, it is only passed to the callback.
</dd>
<dt>ssl_ctx_function</dt><dd>This option does only function for libcurl powered by OpenSSL. If libcurl was built against another SSL library, this functionality is absent.
<br>
Function pointer that should match the following prototype: CURLcode sslctxfun(CURL *curl, void *sslctx, void *parm); This function gets called by libcurl just before the initialization of an SSL connection after having processed all other SSL related options to give a last chance to an application to modify the behaviour of openssl's ssl initialization. The sslctx parameter is actually a pointer to an openssl SSL_CTX. If an error is returned no attempt to establish a connection is made and the perform operation will return the error code from this callback function.  Set the parm argument with the <code>ssl_ctx_data</code> option. This option was introduced in 7.11.0.
<br>
This function will get called on all new connections made to a server, during the SSL negotiation. The SSL_CTX pointer will be a new one every time.
<br>
To use this properly, a non-trivial amount of knowledge of the openssl libraries is necessary. For example, using this function allows you to use openssl callbacks to add additional validation code for certificates, and even to change the actual URI of an HTTPS request (example used in the lib509 test case).  See also the example section for a replacement of the key, certificate and trust file settings.
</dd>
<dt>ssl_ctx_data</dt><dd>Data pointer to pass to the ssl context callback set by the option <code>ssl_ctx_function</code>, this is the pointer you'll get as third parameter, otherwise NULL. (Added in 7.11.0)
</dd>
<dt>conv_to_network_function</dt><dd></dd>
<dt>conv_from_network_function</dt><dd></dd>
<dt>conv_from_utf8_function</dt><dd>Function pointers that should match the following prototype: CURLcode function(stringptr, size_t length);
<br>
These three options apply to non-ASCII platforms only.  They are available only if CURL_DOES_CONVERSIONS was defined when libcurl was built. When this is the case, curl_version_info(3) will return the CURL_VERSION_CONV feature bit set.
<br>
The data to be converted is in a buffer pointed to by the ptr parameter.  The amount of data to convert is indicated by the length parameter.  The converted data overlays the input data in the buffer pointed to by the ptr parameter. CURLE_OK should be returned upon successful conversion.  A CURLcode return value defined by curl.h, such as CURLE_CONV_FAILED, should be returned if an error was encountered.
<br>
<code>conv_to_network_function</code> and <code>conv_from_network_function</code> convert between the host encoding and the network encoding.  They are used when commands or ASCII data are sent/received over the network.
<br>
<code>conv_from_utf</code>8_FUNCTION is called to convert from UTF8 into the host encoding.  It is required only for SSL processing.
<br>
If you set a callback pointer to NULL, or don't set it at all, the built-in libcurl iconv functions will be used.  If HAVE_ICONV was not defined when libcurl was built, and no callback has been established, conversion will return the CURLE_CONV_REQD error code.
<br>
If HAVE_ICONV is defined, CURL_ICONV_CODESET_OF_HOST must also be defined. For example:
<br>
&lt;c2&gt;&lt;a0&gt;#define CURL_ICONV_CODESET_OF_HOST "IBM-1047"
<br>
The iconv code in libcurl will default the network and UTF8 codeset names as follows:
<br>
&lt;c2&gt;&lt;a0&gt;#define CURL_ICONV_CODESET_OF_NETWORK "ISO8859-1"
<br>
&lt;c2&gt;&lt;a0&gt;#define CURL_ICONV_CODESET_FOR_UTF8   "UTF-8"
<br>
You will need to override these definitions if they are different on your system. </dd>
<dt>errorbuffer</dt><dd>Pass a string to a buffer that the libcurl may store human readable error messages in. This may be more helpful than just the return code from curl_easy_perform. The buffer must be at least CURL_ERROR_SIZE big. Although this argument is a 'string', it does not describe an input string. Therefore the (probably undefined) contents of the buffer is NOT copied by the library. You should keep the associated storage available until libcurl no longer needs it. Failing to do so will cause very odd behavior or even crashes. libcurl will need it until you call curl_easy_cleanup(3) or you set the same option again to use a different pointer.
<br>
Use <code>verbose</code> and <code>debugfunction</code> to better debug/trace why errors happen.
<br>
If the library does not return an error, the buffer may not have been touched. Do not rely on the contents in those cases.
<br>
</dd>
<dt>stderr</dt><dd>Pass a FILE * as parameter. Tell libcurl to use this stream instead of stderr when showing the progress meter and displaying <code>verbose</code> data.
</dd>
<dt>failonerror</dt><dd>A parameter set to 1 tells the library to fail silently if the HTTP code returned is equal to or larger than 400. The default action would be to return the page normally, ignoring that code.
<br>
This method is not fail-safe and there are occasions where non-successful response codes will slip through, especially when authentication is involved (response codes 401 and 407).
<br>
You might get some amounts of headers transferred before this situation is detected, like when a "100-continue" is received as a response to a POST/PUT and a 401 or 407 is received immediately afterwards. </dd>
<dt>url</dt><dd>The actual URL to deal with. The parameter should be a string to a zero terminated string.
<br>
If the given URL lacks the protocol part ("http://" or "ftp://" etc), it will attempt to guess which protocol to use based on the given host name. If the given protocol of the set URL is not supported, libcurl will return on error (CURLE_UNSUPPORTED_PROTOCOL) when you call curl_easy_perform(3) or curl_multi_perform(3). Use curl_version_info(3) for detailed info on which protocols are supported.
<br>
The string given to <code>url</code> must be url-encoded and follow RFC 2396 (http://curl.haxx.se/rfc/rfc2396.txt).
<br>
<code>url</code> is the only option that must be set before curl_easy_perform(3) is called.
<br>
<code>protocols</code> can be used to limit what protocols libcurl will use for this transfer, independent of what libcurl has been compiled to support. That may be useful if you accept the URL from an external source and want to limit the accessibility.
</dd>
<dt>protocols</dt><dd>Pass a long that holds a bitmask of CURLPROTO_* defines. If used, this bitmask limits what protocols libcurl may use in the transfer. This allows you to have a libcurl built to support a wide range of protocols but still limit specific transfers to only be allowed to use a subset of them. By default libcurl will accept all protocols it supports. See also <code>redir_protocols</code>. (Added in 7.19.4)
</dd>
<dt>redir_protocols</dt><dd>Pass a long that holds a bitmask of CURLPROTO_* defines. If used, this bitmask limits what protocols libcurl may use in a transfer that it follows to in a redirect when <code>followlocation</code> is enabled. This allows you to limit specific transfers to only be allowed to use a subset of protocols in redirections. By default libcurl will allow all protocols except for FILE and SCP. This is a difference compared to pre-7.19.4 versions which unconditionally would follow to all protocols supported. (Added in 7.19.4)
</dd>
<dt>proxy</dt><dd>Set HTTP proxy to use. The parameter should be a string to a zero terminated string holding the host name or dotted IP address. To specify port number in this string, append :[port] to the end of the host name. The proxy string may be prefixed with [protocol]:// since any such prefix will be ignored. The proxy's port number may optionally be specified with the separate option. If not specified, libcurl will default to using port 1080 for proxies. <code>proxyport</code>.
<br>
When you tell the library to use an HTTP proxy, libcurl will transparently convert operations to HTTP even if you specify an FTP URL etc. This may have an impact on what other features of the library you can use, such as <code>quote</code> and similar FTP specifics that don't work unless you tunnel through the HTTP proxy. Such tunneling is activated with <code>httpproxytunnel</code>.
<br>
libcurl respects the environment variables http_proxy, ftp_proxy, all_proxy etc, if any of those are set. The <code>proxy</code> option does however override any possibly set environment variables.
<br>
Setting the proxy string to "" (an empty string) will explicitly disable the use of a proxy, even if there is an environment variable set for it.
<br>
Since 7.14.1, the proxy host string given in environment variables can be specified the exact same way as the proxy can be set with <code>proxy</code>, include protocol prefix (http://) and embedded user + password.
</dd>
<dt>proxyport</dt><dd>Pass a long with this option to set the proxy port to connect to unless it is specified in the proxy string <code>proxy</code>.
</dd>
<dt>proxytype</dt><dd>Pass a long with this option to set type of the proxy. Available options for this are CURLPROXY_HTTP, CURLPROXY_HTTP_1_0 (added in 7.19.4), CURLPROXY_SOCKS4 (added in 7.15.2), CURLPROXY_SOCKS5, CURLPROXY_SOCKS4A (added in 7.18.0) and CURLPROXY_SOCKS5_HOSTNAME (added in 7.18.0). The HTTP type is default. (Added in 7.10)
</dd>
<dt>noproxy</dt><dd>Pass a pointer to a zero terminated string. The should be a comma- separated list of hosts which do not use a proxy, if one is specified.  The only wildcard is a single * character, which matches all hosts, and effectively disables the proxy. Each name in this list is matched as either a domain which contains the hostname, or the hostname itself. For example, local.com would match local.com, local.com:80, and www.local.com, but not www.notlocal.com. (Added in 7.19.4)
</dd>
<dt>httpproxytunnel</dt><dd>Set the parameter to 1 to make the library tunnel all operations through a given HTTP proxy. There is a big difference between using a proxy and to tunnel through it. If you don't know what this means, you probably don't want this tunneling option.
</dd>
<dt>socks5_gssapi_service</dt><dd>Pass a string as parameter to a string holding the name of the service. The default service name for a SOCKS5 server is rcmd/server-fqdn. This option allows you to change it. (Added in 7.19.4)
</dd>
<dt>socks5_gssapi_nec</dt><dd>Pass a long set to 1 to enable or 0 to disable. As part of the gssapi negotiation a protection mode is negotiated. The rfc1961 says in section 4.3/4.4 it should be protected, but the NEC reference implementation does not. If enabled, this option allows the unprotected exchange of the protection mode negotiation. (Added in 7.19.4).
</dd>
<dt>interface</dt><dd>Pass a string as parameter. This sets the interface name to use as outgoing network interface. The name can be an interface name, an IP address, or a host name.
</dd>
<dt>localport</dt><dd>Pass a long. This sets the local port number of the socket used for connection. This can be used in combination with <code>interface</code> and you are recommended to use <code>localportrange</code> as well when this is set. Note that the only valid port numbers are 1 - 65535. (Added in 7.15.2)
</dd>
<dt>localportrange</dt><dd>Pass a long. This is the number of attempts libcurl should make to find a working local port number. It starts with the given <code>localport</code> and adds one to the number for each retry. Setting this to 1 or below will make libcurl do only one try for the exact port number. Note that port numbers by nature are scarce resources that will be busy at times so setting this value to something too low might cause unnecessary connection setup failures. (Added in 7.15.2)
</dd>
<dt>dns_cache_timeout</dt><dd>Pass a long, this sets the timeout in seconds. Name resolves will be kept in memory for this number of seconds. Set to zero to completely disable caching, or set to -1 to make the cached entries remain forever. By default, libcurl caches this info for 60 seconds.
<br>
NOTE: the name resolve functions of various libc implementations don't re-read name server information unless explicitly told so (for example, by calling res_init(3)). This may cause libcurl to keep using the older server even if DHCP has updated the server info, and this may look like a DNS cache issue to the casual libcurl-app user.
</dd>
<dt>dns_use_global_cache</dt><dd>Pass a long. If the value is 1, it tells curl to use a global DNS cache that will survive between easy handle creations and deletions. This is not thread-safe and this will use a global variable.
<br>
WARNING: this option is considered obsolete. Stop using it. Switch over to using the share interface instead! See <code>share</code> and curl_share_init(3).
</dd>
<dt>buffersize</dt><dd>Pass a long specifying your preferred size (in bytes) for the receive buffer in libcurl.  The main point of this would be that the write callback gets called more often and with smaller chunks. This is just treated as a request, not an order. You cannot be guaranteed to actually get the given size. (Added in 7.10)
<br>
This size is by default set as big as possible (CURL_MAX_WRITE_SIZE), so it only makes sense to use this option if you want it smaller.
</dd>
<dt>port</dt><dd>Pass a long specifying what remote port number to connect to, instead of the one specified in the URL or the default port for the used protocol.
</dd>
<dt>tcp_nodelay</dt><dd>Pass a long specifying whether the TCP_NODELAY option should be set or cleared (1 = set, 0 = clear). The option is cleared by default. This will have no effect after the connection has been established.
<br>
Setting this option will disable TCP's Nagle algorithm. The purpose of this algorithm is to try to minimize the number of small packets on the network (where "small packets" means TCP segments less than the Maximum Segment Size (MSS) for the network).
<br>
Maximizing the amount of data sent per TCP segment is good because it amortizes the overhead of the send. However, in some cases (most notably telnet or rlogin) small segments may need to be sent without delay. This is less efficient than sending larger amounts of data at a time, and can contribute to congestion on the network if overdone.
</dd>
<dt>address_scope</dt><dd>Pass a long specifying the scope_id value to use when connecting to IPv6 link-local or site-local addresses. (Added in 7.19.0) </dd>
<dt>netrc</dt><dd>This parameter controls the preference of libcurl between using user names and passwords from your ~/.netrc file, relative to user names and passwords in the URL supplied with <code>url</code>.
<br>
libcurl uses a user name (and supplied or prompted password) supplied with <code>userpwd</code> in preference to any of the options controlled by this parameter.
<br>
Pass a long, set to one of the values described below.
<br>
CURL_NETRC_OPTIONAL
<br>
The use of your ~/.netrc file is optional, and information in the URL is to be preferred.  The file will be scanned for the host and user name (to find the password only) or for the host only, to find the first user name and password after that machine, which ever information is not specified in the URL.
<br>
Undefined values of the option will have this effect.
<br>
CURL_NETRC_IGNORED
<br>
The library will ignore the file and use only the information in the URL.
<br>
This is the default.
<br>
CURL_NETRC_REQUIRED
<br>
This value tells the library that use of the file is required, to ignore the information in the URL, and to search the file for the host only.
<br>
Only machine name, user name and password are taken into account (init macros and similar things aren't supported).
<br>
libcurl does not verify that the file has the correct properties set (as the standard Unix ftp client does). It should only be readable by user.
</dd>
<dt>netrc_file</dt><dd>Pass a string as parameter, pointing to a zero terminated string containing the full path name to the file you want libcurl to use as .netrc file. If this option is omitted, and <code>netrc</code> is set, libcurl will attempt to find a .netrc file in the current user's home directory. (Added in 7.10.9)
</dd>
<dt>userpwd</dt><dd>Pass a string as parameter, which should be [user name]:[password] to use for the connection. Use <code>httpauth</code> to decide the authentication method.
<br>
When using NTLM, you can set the domain by prepending it to the user name and separating the domain and name with a forward (/) or backward slash (). Like this: "domain/user:password" or "domainuser:password". Some HTTP servers (on Windows) support this style even for Basic authentication.
<br>
When using HTTP and <code>followlocation</code>, libcurl might perform several requests to possibly different hosts. libcurl will only send this user and password information to hosts using the initial host name (unless <code>unrestricted_auth</code> is set), so if libcurl follows locations to other hosts it will not send the user and password to those. This is enforced to prevent accidental information leakage.
</dd>
<dt>proxyuserpwd</dt><dd>Pass a string as parameter, which should be [user name]:[password] to use for the connection to the HTTP proxy.  Use <code>proxyauth</code> to decide the authentication method.
</dd>
<dt>username</dt><dd>Pass a string as parameter, which should be pointing to the zero terminated user name to use for the transfer.
<br>
<code>username</code> sets the user name to be used in protocol authentication. You should not use this option together with the (older) <code>userpwd</code> option.
<br>
In order to specify the password to be used in conjunction with the user name use the <code>password</code> option.  (Added in 7.19.1)
</dd>
<dt>password</dt><dd>Pass a string as parameter, which should be pointing to the zero terminated password to use for the transfer.
<br>
The <code>password</code> option should be used in conjunction with the <code>username</code> option. (Added in 7.19.1)
</dd>
<dt>proxyusername</dt><dd>Pass a string as parameter, which should be pointing to the zero terminated user name to use for the transfer while connecting to Proxy.
<br>
The <code>proxyusername</code> option should be used in same way as the <code>proxyuserpwd</code> is used.  In comparison to <code>proxyuserpwd</code> the <code>proxyusername</code> allows the username to contain a colon, like in the following example: "sip:user@example.com". Note the <code>proxyusername</code> option is an alternative way to set the user name while connecting to Proxy.  There is no meaning to use it together with the <code>proxyuserpwd</code> option.
<br>
In order to specify the password to be used in conjunction with the user name use the <code>proxypassword</code> option.  (Added in 7.19.1)
</dd>
<dt>proxypassword</dt><dd>Pass a string as parameter, which should be pointing to the zero terminated password to use for the transfer while connecting to Proxy.
<br>
The <code>proxypassword</code> option should be used in conjunction with the <code>proxyusername</code> option. (Added in 7.19.1)
</dd>
<dt>httpauth</dt><dd>Pass a long as parameter, which is set to a bitmask, to tell libcurl which authentication method(s) you want it to use. The available bits are listed below. If more than one bit is set, libcurl will first query the site to see which authentication methods it supports and then pick the best one you allow it to use. For some methods, this will induce an extra network round-trip. Set the actual name and password with the <code>userpwd</code> option or with the <code>username</code> and the <code>userpassword</code> options. (Added in 7.10.6)
<br>
CURLAUTH_BASIC
<br>
HTTP Basic authentication. This is the default choice, and the only method that is in wide-spread use and supported virtually everywhere. This sends the user name and password over the network in plain text, easily captured by others.
<br>
CURLAUTH_DIGEST
<br>
HTTP Digest authentication.  Digest authentication is defined in RFC2617 and is a more secure way to do authentication over public networks than the regular old-fashioned Basic method.
<br>
CURLAUTH_DIGEST_IE
<br>
HTTP Digest authentication with an IE flavor.  Digest authentication is defined in RFC2617 and is a more secure way to do authentication over public networks than the regular old-fashioned Basic method. The IE flavor is simply that libcurl will use a special "quirk" that IE is known to have used before version 7 and that some servers require the client to use. (This define was added in 7.19.3)
<br>
CURLAUTH_GSSNEGOTIATE
<br>
HTTP GSS-Negotiate authentication. The GSS-Negotiate (also known as plain "Negotiate") method was designed by Microsoft and is used in their web applications. It is primarily meant as a support for Kerberos5 authentication but may also be used along with other authentication methods. For more information see IETF draft draft-brezak-spnego-http-04.txt.
<br>
You need to build libcurl with a suitable GSS-API library for this to work.
<br>
CURLAUTH_NTLM
<br>
HTTP NTLM authentication. A proprietary protocol invented and used by Microsoft. It uses a challenge-response and hash concept similar to Digest, to prevent the password from being eavesdropped.
<br>
You need to build libcurl with OpenSSL support for this option to work, or build libcurl on Windows.
<br>
CURLAUTH_ANY
<br>
This is a convenience macro that sets all bits and thus makes libcurl pick any it finds suitable. libcurl will automatically select the one it finds most secure.
<br>
CURLAUTH_ANYSAFE
<br>
This is a convenience macro that sets all bits except Basic and thus makes libcurl pick any it finds suitable. libcurl will automatically select the one it finds most secure.
<br>
</dd>
<dt>proxyauth</dt><dd>Pass a long as parameter, which is set to a bitmask, to tell libcurl which authentication method(s) you want it to use for your proxy authentication.  If more than one bit is set, libcurl will first query the site to see what authentication methods it supports and then pick the best one you allow it to use. For some methods, this will induce an extra network round-trip. Set the actual name and password with the <code>proxyuserpwd</code> option. The bitmask can be constructed by or'ing together the bits listed above for the <code>httpauth</code> option. As of this writing, only Basic, Digest and NTLM work. (Added in 7.10.7) </dd>
<dt>autoreferer</dt><dd>Pass a parameter set to 1 to enable this. When enabled, libcurl will automatically set the Referer: field in requests where it follows a Location: redirect.
</dd>
<dt>encoding</dt><dd>Sets the contents of the Accept-Encoding: header sent in an HTTP request, and enables decoding of a response when a Content-Encoding: header is received. Three encodings are supported: identity, which does nothing, deflate which requests the server to compress its response using the zlib algorithm, and gzip which requests the gzip algorithm.  If a zero-length string is set, then an Accept-Encoding: header containing all supported encodings is sent.
<br>
This is a request, not an order; the server may or may not do it.  This option must be set (to any non-NULL value) or else any unsolicited encoding done by the server is ignored. See the special file lib/README.encoding for details.
</dd>
<dt>followlocation</dt><dd>A parameter set to 1 tells the library to follow any Location: header that the server sends as part of an HTTP header.
<br>
This means that the library will re-send the same request on the new location and follow new Location: headers all the way until no more such headers are returned. <code>maxredirs</code> can be used to limit the number of redirects libcurl will follow.
<br>
NOTE: since 7.19.4, libcurl can limit to what protocols it will automatically follow. The accepted protocols are set with <code>redir_protocols</code> and it excludes the FILE protocol by default.
</dd>
<dt>unrestricted_auth</dt><dd>A parameter set to 1 tells the library it can continue to send authentication (user+password) when following locations, even when hostname changed. This option is meaningful only when setting <code>followlocation</code>.
</dd>
<dt>maxredirs</dt><dd>Pass a long. The set number will be the redirection limit. If that many redirections have been followed, the next redirect will cause an error (CURLE_TOO_MANY_REDIRECTS). This option only makes sense if the <code>followlocation</code> is used at the same time. Added in 7.15.1: Setting the limit to 0 will make libcurl refuse any redirect. Set it to -1 for an infinite number of redirects (which is the default)
</dd>
<dt>postredir</dt><dd>Pass a bitmask to control how libcurl acts on redirects after POSTs that get a 301 or 302 response back.  A parameter with bit 0 set (value CURL_REDIR_POST_301) tells the library to respect RFC 2616/10.3.2 and not convert POST requests into GET requests when following a 301 redirection. Setting bit 1 (value CURL_REDIR_POST_302) makes libcurl maintain the request method after a 302 redirect. CURL_REDIR_POST_ALL is a convenience define that sets both bits.
<br>
The non-RFC behaviour is ubiquitous in web browsers, so the library does the conversion by default to maintain consistency. However, a server may require a POST to remain a POST after such a redirection. This option is meaningful only when setting <code>followlocation</code>.  (Added in 7.17.1) (This option was known as <code>post</code>301 up to 7.19.0 as it only supported the 301 way before then)
</dd>
<dt>put</dt><dd>A parameter set to 1 tells the library to use HTTP PUT to transfer data. The data should be set with <code>readdata</code> and <code>infilesize</code>.
<br>
This option is deprecated and starting with version 7.12.1 you should instead use <code>upload</code>.
</dd>
<dt>post</dt><dd>A parameter set to 1 tells the library to do a regular HTTP post. This will also make the library use a "Content-Type: application/x-www-form-urlencoded" header. (This is by far the most commonly used POST method).
<br>
Use one of <code>postfields</code> or <code>copypostfields</code> options to specify what data to post and <code>postfieldsize</code> or <code>postfieldsize_large</code> to set the data size.
<br>
Optionally, you can provide data to POST using the <code>readfunction</code> and <code>readdata</code> options but then you must make sure to not set <code>postfields</code> to anything but NULL. When providing data with a callback, you must transmit it using chunked transfer-encoding or you must set the size of the data with the <code>postfieldsize</code> or <code>postfieldsize_large</code> option. To enable chunked encoding, you simply pass in the appropriate Transfer-Encoding header, see the post-callback.c example.
<br>
You can override the default POST Content-Type: header by setting your own with <code>httpheader</code>.
<br>
Using POST with HTTP 1.1 implies the use of a "Expect: 100-continue" header. You can disable this header with <code>httpheader</code> as usual.
<br>
If you use POST to a HTTP 1.1 server, you can send data without knowing the size before starting the POST if you use chunked encoding. You enable this by adding a header like "Transfer-Encoding: chunked" with <code>httpheader</code>. With HTTP 1.0 or without chunked transfer, you must specify the size in the request.
<br>
When setting <code>post</code> to 1, it will automatically set <code>nobody</code> to 0 (since 7.14.1).
<br>
If you issue a POST request and then want to make a HEAD or GET using the same re-used handle, you must explicitly set the new request type using <code>nobody</code> or <code>httpget</code> or similar.
</dd>
<dt>postfields</dt><dd>Pass a void * as parameter, which should be the full data to post in an HTTP POST operation. You must make sure that the data is formatted the way you want the server to receive it. libcurl will not convert or encode it for you. Most web servers will assume this data to be url-encoded. Take note.
<br>
The pointed data are NOT copied by the library: as a consequence, they must be preserved by the calling application until the transfer finishes.
<br>
This POST is a normal application/x-www-form-urlencoded kind (and libcurl will set that Content-Type by default when this option is used), which is the most commonly used one by HTML forms. See also the <code>post</code>. Using <code>postfields</code> implies <code>post</code>.
<br>
If you want to do a zero-byte POST, you need to set <code>postfieldsize</code> explicitly to zero, as simply setting <code>postfields</code> to NULL or "" just effectively disables the sending of the specified string. libcurl will instead assume that you'll send the POST data using the read callback!
<br>
Using POST with HTTP 1.1 implies the use of a "Expect: 100-continue" header. You can disable this header with <code>httpheader</code> as usual.
<br>
To make multipart/formdata posts (aka rfc1867-posts), check out the <code>httppost</code> option.
</dd>
<dt>postfieldsize</dt><dd>If you want to post data to the server without letting libcurl do a strlen() to measure the data size, this option must be used. When this option is used you can post fully binary data, which otherwise is likely to fail. If this size is set to -1, the library will use strlen() to get the size.
</dd>
<dt>postfieldsize_large</dt><dd>Pass a curl_off_t as parameter. Use this to set the size of the <code>postfields</code> data to prevent libcurl from doing strlen() on the data to figure out the size. This is the large file version of the <code>postfieldsize</code> option. (Added in 7.11.1)
</dd>
<dt>copypostfields</dt><dd>Pass a string as parameter, which should be the full data to post in an HTTP POST operation. It behaves as the <code>postfields</code> option, but the original data are copied by the library, allowing the application to overwrite the original data after setting this option.
<br>
Because data are copied, care must be taken when using this option in conjunction with <code>postfieldsize</code> or <code>postfieldsize_large</code>: If the size has not been set prior to <code>copypostfields</code>, the data are assumed to be a NUL-terminated string; else the stored size informs the library about the data byte count to copy. In any case, the size must not be changed after <code>copypostfields</code>, unless another <code>postfields</code> or <code>copypostfields</code> option is issued. (Added in 7.17.1)
</dd>
<dt>httppost</dt><dd>Tells libcurl you want a multipart/formdata HTTP POST to be made and you instruct what data to pass on to the server.  Pass a pointer to a linked list of curl_httppost structs as parameter.  The easiest way to create such a list, is to use curl_formadd(3) as documented. The data in this list must remain intact until you close this curl handle again with curl_easy_cleanup(3).
<br>
Using POST with HTTP 1.1 implies the use of a "Expect: 100-continue" header. You can disable this header with <code>httpheader</code> as usual.
<br>
When setting <code>httppost</code>, it will automatically set <code>nobody</code> to 0 (since 7.14.1).
</dd>
<dt>referer</dt><dd>Pass a pointer to a zero terminated string as parameter. It will be used to set the Referer: header in the http request sent to the remote server. This can be used to fool servers or scripts. You can also set any custom header with <code>httpheader</code>.
</dd>
<dt>useragent</dt><dd>Pass a pointer to a zero terminated string as parameter. It will be used to set the User-Agent: header in the http request sent to the remote server. This can be used to fool servers or scripts. You can also set any custom header with <code>httpheader</code>.
</dd>
<dt>httpheader</dt><dd>Pass a pointer to a linked list of HTTP headers to pass to the server in your HTTP request. The linked list should be a fully valid list of struct curl_slist structs properly filled in. Use curl_slist_append(3) to create the list and curl_slist_free_all(3) to clean up an entire list. If you add a header that is otherwise generated and used by libcurl internally, your added one will be used instead. If you add a header with no content as in 'Accept:' (no data on the right side of the colon), the internally used header will get disabled. Thus, using this option you can add new headers, replace internal headers and remove internal headers. To add a header with no content, make the content be two quotes: "". The headers included in the linked list must not be CRLF-terminated, because curl adds CRLF after each header item. Failure to comply with this will result in strange bugs because the server will most likely ignore part of the headers you specified.
<br>
The first line in a request (containing the method, usually a GET or POST) is not a header and cannot be replaced using this option. Only the lines following the request-line are headers. Adding this method line in this list of headers will only cause your request to send an invalid header.
<br>
Pass a NULL to this to reset back to no custom headers.
<br>
The most commonly replaced headers have "shortcuts" in the options <code>cookie</code>, <code>useragent</code> and <code>referer</code>.
</dd>
<dt>http200aliases</dt><dd>Pass a pointer to a linked list of aliases to be treated as valid HTTP 200 responses.  Some servers respond with a custom header response line.  For example, IceCast servers respond with "ICY 200 OK".  By including this string in your list of aliases, the response will be treated as a valid HTTP header line such as "HTTP/1.0 200 OK". (Added in 7.10.3)
<br>
The linked list should be a fully valid list of struct curl_slist structs, and be properly filled in.  Use curl_slist_append(3) to create the list and curl_slist_free_all(3) to clean up an entire list.
<br>
The alias itself is not parsed for any version strings. Before libcurl 7.16.3, Libcurl used the value set by option <code>http_version</code>, but starting with 7.16.3 the protocol is assumed to match HTTP 1.0 when an alias matched.
</dd>
<dt>cookie</dt><dd>Pass a pointer to a zero terminated string as parameter. It will be used to set a cookie in the http request. The format of the string should be NAME=CONTENTS, where NAME is the cookie name and CONTENTS is what the cookie should contain.
<br>
If you need to set multiple cookies, you need to set them all using a single option and thus you need to concatenate them all in one single string. Set multiple cookies in one string like this: "name1=content1; name2=content2;" etc.
<br>
Note that this option sets the cookie header explictly in the outgoing request(s). If multiple requests are done due to authentication, followed redirections or similar, they will all get this cookie passed on.
<br>
Using this option multiple times will only make the latest string override the previous ones.
</dd>
<dt>cookiefile</dt><dd>Pass a pointer to a zero terminated string as parameter. It should contain the name of your file holding cookie data to read. The cookie data may be in Netscape / Mozilla cookie data format or just regular HTTP-style headers dumped to a file.
<br>
Given an empty or non-existing file or by passing the empty string (""), this option will enable cookies for this curl handle, making it understand and parse received cookies and then use matching cookies in future requests.
<br>
If you use this option multiple times, you just add more files to read. Subsequent files will add more cookies.
</dd>
<dt>cookiejar</dt><dd>Pass a file name as string, zero terminated. This will make libcurl write all internally known cookies to the specified file when curl_easy_cleanup(3) is called. If no cookies are known, no file will be created. Specify "-" to instead have the cookies written to stdout. Using this option also enables cookies for this session, so if you for example follow a location it will make matching cookies get sent accordingly.
<br>
If the cookie jar file can't be created or written to (when the curl_easy_cleanup(3) is called), libcurl will not and cannot report an error for this. Using <code>verbose</code> or <code>debugfunction</code> will get a warning to display, but that is the only visible feedback you get about this possibly lethal situation.
</dd>
<dt>cookiesession</dt><dd>Pass a long set to 1 to mark this as a new cookie "session". It will force libcurl to ignore all cookies it is about to load that are "session cookies" from the previous session. By default, libcurl always stores and loads all cookies, independent if they are session cookies or not. Session cookies are cookies without expiry date and they are meant to be alive and existing for this "session" only.
</dd>
<dt>cookielist</dt><dd>Pass a string to a cookie string. Cookie can be either in Netscape / Mozilla format or just regular HTTP-style header (Set-Cookie: ...) format. If cURL cookie engine was not enabled it will enable its cookie engine.  Passing a magic string "ALL" will erase all cookies known by cURL. (Added in 7.14.1) Passing the special string "SESS" will only erase all session cookies known by cURL. (Added in 7.15.4) Passing the special string "FLUSH" will write all cookies known by cURL to the file specified by <code>cookiejar</code>. (Added in 7.17.1)
</dd>
<dt>httpget</dt><dd>Pass a long. If the long is 1, this forces the HTTP request to get back to GET. Usable if a POST, HEAD, PUT, or a custom request has been used previously using the same curl handle.
<br>
When setting <code>httpget</code> to 1, it will automatically set <code>nobody</code> to 0 (since 7.14.1).
</dd>
<dt>http_version</dt><dd>Pass a long, set to one of the values described below. They force libcurl to use the specific HTTP versions. This is not sensible to do unless you have a good reason.
<br>
CURL_HTTP_VERSION_NONE
<br>
We don't care about what version the library uses. libcurl will use whatever it thinks fit.
<br>
CURL_HTTP_VERSION_1_0
<br>
Enforce HTTP 1.0 requests.
<br>
CURL_HTTP_VERSION_1_1
<br>
Enforce HTTP 1.1 requests.
<br>
</dd>
<dt>ignore_content_length</dt><dd>Ignore the Content-Length header. This is useful for Apache 1.x (and similar servers) which will report incorrect content length for files over 2 gigabytes. If this option is used, curl will not be able to accurately report progress, and will simply stop the download when the server ends the connection. (added in 7.14.1)
</dd>
<dt>http_content_decoding</dt><dd>Pass a long to tell libcurl how to act on content decoding. If set to zero, content decoding will be disabled. If set to 1 it is enabled. Note however that libcurl has no default content decoding but requires you to use <code>encoding</code> for that. (added in 7.16.2)
</dd>
<dt>http_transfer_decoding</dt><dd>Pass a long to tell libcurl how to act on transfer decoding. If set to zero, transfer decoding will be disabled, if set to 1 it is enabled (default). libcurl does chunked transfer decoding by default unless this option is set to zero. (added in 7.16.2) </dd>
<dt>tftpblksize</dt><dd>Specify block size to use for TFTP data transmission. Valid range as per RFC 2348 is 8-65464 bytes. The default of 512 bytes will be used if this option is not specified. The specified block size will only be used pending support by the remote server. If the server does not return an option acknowledgement or returns an option acknowledgement with no blksize, the default of 512 bytes will be used. (added in 7.19.4) </dd>
<dt>ftpport</dt><dd>Pass a pointer to a zero terminated string as parameter. It will be used to get the IP address to use for the FTP PORT instruction. The PORT instruction tells the remote server to connect to our specified IP address. The string may be a plain IP address, a host name, a network interface name (under Unix) or just a '-' symbol to let the library use your system's default IP address. Default FTP operations are passive, and thus won't use PORT.
<br>
You disable PORT again and go back to using the passive version by setting this option to NULL.
</dd>
<dt>quote</dt><dd>Pass a pointer to a linked list of FTP or SFTP commands to pass to the server prior to your FTP request. This will be done before any other commands are issued (even before the CWD command for FTP). The linked list should be a fully valid list of 'struct curl_slist' structs properly filled in with text strings. Use curl_slist_append(3) to append strings (commands) to the list, and clear the entire list afterwards with curl_slist_free_all(3). Disable this operation again by setting a NULL to this option. The set of valid FTP commands depends on the server (see RFC959 for a list of mandatory commands). The valid SFTP commands are: chgrp, chmod, chown, ln, mkdir, pwd, rename, rm, rmdir, symlink (see curl (1)) (SFTP support added in 7.16.3)
</dd>
<dt>postquote</dt><dd>Pass a pointer to a linked list of FTP or SFTP commands to pass to the server after your FTP transfer request. The commands will only be run if no error occurred. The linked list should be a fully valid list of struct curl_slist structs properly filled in as described for <code>quote</code>. Disable this operation again by setting a NULL to this option.
</dd>
<dt>prequote</dt><dd>Pass a pointer to a linked list of FTP commands to pass to the server after the transfer type is set. The linked list should be a fully valid list of struct curl_slist structs properly filled in as described for <code>quote</code>. Disable this operation again by setting a NULL to this option. Before version 7.15.6, if you also set <code>nobody</code> to 1, this option didn't work.
</dd>
<dt>dirlistonly</dt><dd>A parameter set to 1 tells the library to just list the names of files in a directory, instead of doing a full directory listing that would include file sizes, dates etc. This works for FTP and SFTP URLs.
<br>
This causes an FTP NLST command to be sent on an FTP server.  Beware that some FTP servers list only files in their response to NLST; they might not include subdirectories and symbolic links.
<br>
(This option was known as <code>ftplistonly</code> up to 7.16.4)
</dd>
<dt>append</dt><dd>A parameter set to 1 tells the library to append to the remote file instead of overwrite it. This is only useful when uploading to an FTP site.
<br>
(This option was known as <code>ftpappend</code> up to 7.16.4)
</dd>
<dt>ftp_use_eprt</dt><dd>Pass a long. If the value is 1, it tells curl to use the EPRT (and LPRT) command when doing active FTP downloads (which is enabled by <code>ftpport</code>). Using EPRT means that it will first attempt to use EPRT and then LPRT before using PORT, but if you pass zero to this option, it will not try using EPRT or LPRT, only plain PORT. (Added in 7.10.5)
<br>
If the server is an IPv6 host, this option will have no effect as of 7.12.3.
</dd>
<dt>ftp_use_epsv</dt><dd>Pass a long. If the value is 1, it tells curl to use the EPSV command when doing passive FTP downloads (which it always does by default). Using EPSV means that it will first attempt to use EPSV before using PASV, but if you pass zero to this option, it will not try using EPSV, only plain PASV.
<br>
If the server is an IPv6 host, this option will have no effect as of 7.12.3.
</dd>
<dt>ftp_create_missing_dirs</dt><dd>Pass a long. If the value is 1, curl will attempt to create any remote directory that it fails to CWD into. CWD is the command that changes working directory. (Added in 7.10.7)
<br>
This setting also applies to SFTP-connections. curl will attempt to create the remote directory if it can't obtain a handle to the target-location. The creation will fail if a file of the same name as the directory to create already exists or lack of permissions prevents creation. (Added in 7.16.3)
<br>
Starting with 7.19.4, you can also set this value to 2, which will make libcurl retry the CWD command again if the subsequent MKD command fails. This is especially useful if you're doing many simultanoes connections against the same server and they all have this option enabled, as then CWD may first fail but then another connection does MKD before this connection and thus MKD fails but trying CWD works! 7.19.4 also introduced the CURLFTP_CREATE_DIR and CURLFTP_CREATE_DIR_RETRY enum names for these arguments.
<br>
Before version 7.19.4, libcurl will simply ignore arguments set to 2 and act as if 1 was selected.
</dd>
<dt>ftp_response_timeout</dt><dd>Pass a long.  Causes curl to set a timeout period (in seconds) on the amount of time that the server is allowed to take in order to generate a response message for a command before the session is considered hung.  While curl is waiting for a response, this value overrides <code>timeout</code>. It is recommended that if used in conjunction with <code>timeout</code>, you set <code>ftp_response_timeout</code> to a value smaller than <code>timeout</code>.  (Added in 7.10.8)
</dd>
<dt>ftp_alternative_to_user</dt><dd>Pass a string as parameter, pointing to a string which will be used to authenticate if the usual FTP "USER user" and "PASS password" negotiation fails. This is currently only known to be required when connecting to Tumbleweed's Secure Transport FTPS server using client certificates for authentication. (Added in 7.15.5)
</dd>
<dt>ftp_skip_pasv_ip</dt><dd>Pass a long. If set to 1, it instructs libcurl to not use the IP address the server suggests in its 227-response to libcurl's PASV command when libcurl connects the data connection. Instead libcurl will re-use the same IP address it already uses for the control connection. But it will use the port number from the 227-response. (Added in 7.14.2)
<br>
This option has no effect if PORT, EPRT or EPSV is used instead of PASV.
</dd>
<dt>use_ssl</dt><dd>Pass a long using one of the values from below, to make libcurl use your desired level of SSL for the FTP transfer. (Added in 7.11.0)
<br>
(This option was known as <code>ftp_ssl</code> up to 7.16.4, and the constants were known as CURLFTPSSL_*)
<br>
CURLUSESSL_NONE
<br>
Don't attempt to use SSL.
<br>
CURLUSESSL_TRY
<br>
Try using SSL, proceed as normal otherwise.
<br>
CURLUSESSL_CONTROL
<br>
Require SSL for the control connection or fail with CURLE_USE_SSL_FAILED.
<br>
CURLUSESSL_ALL
<br>
Require SSL for all communication or fail with CURLE_USE_SSL_FAILED.
<br>
</dd>
<dt>ftpsslauth</dt><dd>Pass a long using one of the values from below, to alter how libcurl issues "AUTH TLS" or "AUTH SSL" when FTP over SSL is activated (see <code>use_ssl</code>). (Added in 7.12.2)
<br>
CURLFTPAUTH_DEFAULT
<br>
Allow libcurl to decide.
<br>
CURLFTPAUTH_SSL
<br>
Try "AUTH SSL" first, and only if that fails try "AUTH TLS".
<br>
CURLFTPAUTH_TLS
<br>
Try "AUTH TLS" first, and only if that fails try "AUTH SSL".
<br>
</dd>
<dt>ftp_ssl_ccc</dt><dd>If enabled, this option makes libcurl use CCC (Clear Command Channel). It shuts down the SSL/TLS layer after authenticating. The rest of the control channel communication will be unencrypted. This allows NAT routers to follow the FTP transaction. Pass a long using one of the values below. (Added in 7.16.1)
<br>
CURLFTPSSL_CCC_NONE
<br>
Don't attempt to use CCC.
<br>
CURLFTPSSL_CCC_PASSIVE
<br>
Do not initiate the shutdown, but wait for the server to do it. Do not send a reply.
<br>
CURLFTPSSL_CCC_ACTIVE
<br>
Initiate the shutdown and wait for a reply.
<br>
</dd>
<dt>ftp_account</dt><dd>Pass a pointer to a zero-terminated string (or NULL to disable). When an FTP server asks for "account data" after user name and password has been provided, this data is sent off using the ACCT command. (Added in 7.13.0)
</dd>
<dt>ftp_filemethod</dt><dd>Pass a long that should have one of the following values. This option controls what method libcurl should use to reach a file on a FTP(S) server. The argument should be one of the following alternatives:
<br>
CURLFTPMETHOD_MULTICWD
<br>
libcurl does a single CWD operation for each path part in the given URL. For deep hierarchies this means many commands. This is how RFC1738 says it should be done. This is the default but the slowest behavior.
<br>
CURLFTPMETHOD_NOCWD
<br>
libcurl does no CWD at all. libcurl will do SIZE, RETR, STOR etc and give a full path to the server for all these commands. This is the fastest behavior.
<br>
CURLFTPMETHOD_SINGLECWD
<br>
libcurl does one CWD with the full target directory and then operates on the file "normally" (like in the multicwd case). This is somewhat more standards compliant than 'nocwd' but without the full penalty of 'multicwd'.
<br>
(Added in 7.15.1) </dd>
<dt>transfertext</dt><dd>A parameter set to 1 tells the library to use ASCII mode for FTP transfers, instead of the default binary transfer. For win32 systems it does not set the stdout to binary mode. This option can be usable when transferring text data between systems with different views on certain characters, such as newlines or similar.
<br>
libcurl does not do a complete ASCII conversion when doing ASCII transfers over FTP. This is a known limitation/flaw that nobody has rectified. libcurl simply sets the mode to ASCII and performs a standard transfer.
</dd>
<dt>proxy_transfer_mode</dt><dd>Pass a long. If the value is set to 1 (one), it tells libcurl to set the transfer mode (binary or ASCII) for FTP transfers done via an HTTP proxy, by appending ;type=a or ;type=i to the URL. Without this setting, or it being set to 0 (zero, the default), <code>transfertext</code> has no effect when doing FTP via a proxy. Beware that not all proxies support this feature.  (Added in 7.18.0)
</dd>
<dt>crlf</dt><dd>Convert Unix newlines to CRLF newlines on transfers.
</dd>
<dt>range</dt><dd>Pass a string as parameter, which should contain the specified range you want. It should be in the format "X-Y", where X or Y may be left out. HTTP transfers also support several intervals, separated with commas as in "X-Y,N-M". Using this kind of multiple intervals will cause the HTTP server to send the response document in pieces (using standard MIME separation techniques). Pass a NULL to this option to disable the use of ranges.
<br>
Ranges work on HTTP, FTP and FILE (since 7.18.0) transfers only.
</dd>
<dt>resume_from</dt><dd>Pass a long as parameter. It contains the offset in number of bytes that you want the transfer to start from. Set this option to 0 to make the transfer start from the beginning (effectively disabling resume). For FTP, set this option to -1 to make the transfer start from the end of the target file (useful to continue an interrupted upload).
</dd>
<dt>resume_from_large</dt><dd>Pass a curl_off_t as parameter. It contains the offset in number of bytes that you want the transfer to start from. (Added in 7.11.0)
</dd>
<dt>customrequest</dt><dd>Pass a pointer to a zero terminated string as parameter. It will be used instead of GET or HEAD when doing an HTTP request, or instead of LIST or NLST when doing a FTP directory listing. This is useful for doing DELETE or other more or less obscure HTTP requests. Don't do this at will, make sure your server supports the command first.
<br>
When you change the request method by setting <code>customrequest</code> to something, you don't actually change how libcurl behaves or acts in regards to the particular request method, it will only change the actual string sent in the request.
<br>
For example: if you tell libcurl to do a HEAD request, but then change the request to a "GET" with <code>customrequest</code> you'll still see libcurl act as if it sent a HEAD even when it does send a GET.
<br>
To switch to a proper HEAD, use <code>nobody</code>, to switch to a proper POST, use <code>post</code> or <code>postfields</code> and so on.
<br>
Restore to the internal default by setting this to NULL.
<br>
Many people have wrongly used this option to replace the entire request with their own, including multiple headers and POST contents. While that might work in many cases, it will cause libcurl to send invalid requests and it could possibly confuse the remote server badly. Use <code>post</code> and <code>postfields</code> to set POST data. Use <code>httpheader</code> to replace or extend the set of headers sent by libcurl. Use <code>http_version</code> to change HTTP version.
</dd>
<dt>filetime</dt><dd>Pass a long. If it is 1, libcurl will attempt to get the modification date of the remote document in this operation. This requires that the remote server sends the time or replies to a time querying command. The curl_easy_getinfo(3) function with the CURLINFO_FILETIME argument can be used after a transfer to extract the received time (if any).
</dd>
<dt>nobody</dt><dd>A parameter set to 1 tells the library to not include the body-part in the output. This is only relevant for protocols that have separate header and body parts. On HTTP(S) servers, this will make libcurl do a HEAD request.
<br>
To change request to GET, you should use <code>httpget</code>. Change request to POST with <code>post</code> etc.
</dd>
<dt>infilesize</dt><dd>When uploading a file to a remote site, this option should be used to tell libcurl what the expected size of the infile is. This value should be passed as a long. See also <code>infilesize_large</code>.
<br>
For uploading using SCP, this option or <code>infilesize_large</code> is mandatory.
<br>
Note that this option does not limit how much data libcurl will actually send, as that is controlled entirely by what the read callback returns.
</dd>
<dt>infilesize_large</dt><dd>When uploading a file to a remote site, this option should be used to tell libcurl what the expected size of the infile is.  This value should be passed as a curl_off_t. (Added in 7.11.0)
<br>
For uploading using SCP, this option or <code>infilesize</code> is mandatory.
<br>
Note that this option does not limit how much data libcurl will actually send, as that is controlled entirely by what the read callback returns.
</dd>
<dt>upload</dt><dd>A parameter set to 1 tells the library to prepare for an upload. The <code>readdata</code> and <code>infilesize</code> or <code>infilesize_large</code> options are also interesting for uploads. If the protocol is HTTP, uploading means using the PUT request unless you tell libcurl otherwise.
<br>
Using PUT with HTTP 1.1 implies the use of a "Expect: 100-continue" header. You can disable this header with <code>httpheader</code> as usual.
<br>
If you use PUT to a HTTP 1.1 server, you can upload data without knowing the size before starting the transfer if you use chunked encoding. You enable this by adding a header like "Transfer-Encoding: chunked" with <code>httpheader</code>. With HTTP 1.0 or without chunked transfer, you must specify the size.
</dd>
<dt>maxfilesize</dt><dd>Pass a long as parameter. This allows you to specify the maximum size (in bytes) of a file to download. If the file requested is larger than this value, the transfer will not start and CURLE_FILESIZE_EXCEEDED will be returned.
<br>
The file size is not always known prior to download, and for such files this option has no effect even if the file transfer ends up being larger than this given limit. This concerns both FTP and HTTP transfers.
</dd>
<dt>maxfilesize_large</dt><dd>Pass a curl_off_t as parameter. This allows you to specify the maximum size (in bytes) of a file to download. If the file requested is larger than this value, the transfer will not start and CURLE_FILESIZE_EXCEEDED will be returned. (Added in 7.11.0)
<br>
The file size is not always known prior to download, and for such files this option has no effect even if the file transfer ends up being larger than this given limit. This concerns both FTP and HTTP transfers.
</dd>
<dt>timecondition</dt><dd>Pass a long as parameter. This defines how the <code>timevalue</code> time value is treated. You can set this parameter to CURL_TIMECOND_IFMODSINCE or CURL_TIMECOND_IFUNMODSINCE. This feature applies to HTTP and FTP.
<br>
The last modification time of a file is not always known and in such instances this feature will have no effect even if the given time condition would not have been met. curl_easy_getinfo(3) with the CURLINFO_CONDITION_UNMET option can be used after a transfer to learn if a zero-byte successful "transfer" was due to this condition not matching.
</dd>
<dt>timevalue</dt><dd>Pass a long as parameter. This should be the time in seconds since 1 Jan 1970, and the time will be used in a condition as specified with <code>timecondition</code>. </dd>
<dt>timeout</dt><dd>Pass a long as parameter containing the maximum time in seconds that you allow the libcurl transfer operation to take. Normally, name lookups can take a considerable time and limiting operations to less than a few minutes risk aborting perfectly normal operations. This option will cause curl to use the SIGALRM to enable time-outing system calls.
<br>
In unix-like systems, this might cause signals to be used unless <code>nosignal</code> is set.
</dd>
<dt>timeout_ms</dt><dd>Like <code>timeout</code> but takes number of milliseconds instead. If libcurl is built to use the standard system name resolver, that portion of the transfer will still use full-second resolution for timeouts with a minimum timeout allowed of one second. (Added in 7.16.2)
</dd>
<dt>low_speed_limit</dt><dd>Pass a long as parameter. It contains the transfer speed in bytes per second that the transfer should be below during <code>low_speed_time</code> seconds for the library to consider it too slow and abort.
</dd>
<dt>low_speed_time</dt><dd>Pass a long as parameter. It contains the time in seconds that the transfer should be below the <code>low_speed_limit</code> for the library to consider it too slow and abort.
</dd>
<dt>max_send_speed_large</dt><dd>Pass a curl_off_t as parameter.  If an upload exceeds this speed (counted in bytes per second) on cumulative average during the transfer, the transfer will pause to keep the average rate less than or equal to the parameter value. Defaults to unlimited speed. (Added in 7.15.5)
</dd>
<dt>max_recv_speed_large</dt><dd>Pass a curl_off_t as parameter.  If a download exceeds this speed (counted in bytes per second) on cumulative average during the transfer, the transfer will pause to keep the average rate less than or equal to the parameter value. Defaults to unlimited speed. (Added in 7.15.5)
</dd>
<dt>maxconnects</dt><dd>Pass a long. The set number will be the persistent connection cache size. The set amount will be the maximum amount of simultaneously open connections that libcurl may cache in this easy handle. Default is 5, and there isn't much point in changing this value unless you are perfectly aware of how this works and changes libcurl's behaviour. This concerns connections using any of the protocols that support persistent connections.
<br>
When reaching the maximum limit, curl closes the oldest one in the cache to prevent increasing the number of open connections.
<br>
If you already have performed transfers with this curl handle, setting a smaller MAXCONNECTS than before may cause open connections to get closed unnecessarily.
<br>
Note that if you add this easy handle to a multi handle, this setting is not acknowledged, and you must instead use curl_multi_setopt(3) and the CURLMOPT_MAXCONNECTS option.
</dd>
<dt>closepolicy</dt><dd>(Obsolete) This option does nothing.
</dd>
<dt>fresh_connect</dt><dd>Pass a long. Set to 1 to make the next transfer use a new (fresh) connection by force. If the connection cache is full before this connection, one of the existing connections will be closed as according to the selected or default policy. This option should be used with caution and only if you understand what it does. Set this to 0 to have libcurl attempt re-using an existing connection (default behavior).
</dd>
<dt>forbid_reuse</dt><dd>Pass a long. Set to 1 to make the next transfer explicitly close the connection when done. Normally, libcurl keeps all connections alive when done with one transfer in case a succeeding one follows that can re-use them. This option should be used with caution and only if you understand what it does. Set to 0 to have libcurl keep the connection open for possible later re-use (default behavior).
</dd>
<dt>connecttimeout</dt><dd>Pass a long. It should contain the maximum time in seconds that you allow the connection to the server to take.  This only limits the connection phase, once it has connected, this option is of no more use. Set to zero to disable connection timeout (it will then only timeout on the system's internal timeouts). See also the <code>timeout</code> option.
<br>
In unix-like systems, this might cause signals to be used unless <code>nosignal</code> is set.
</dd>
<dt>connecttimeout_ms</dt><dd>Like <code>connecttimeout</code> but takes the number of milliseconds instead. If libcurl is built to use the standard system name resolver, that portion of the connect will still use full-second resolution for timeouts with a minimum timeout allowed of one second. (Added in 7.16.2)
</dd>
<dt>ipresolve</dt><dd>Allows an application to select what kind of IP addresses to use when resolving host names. This is only interesting when using host names that resolve addresses using more than one version of IP. The allowed values are:
<br>
CURL_IPRESOLVE_WHATEVER
<br>
Default, resolves addresses to all IP versions that your system allows.
<br>
CURL_IPRESOLVE_V4
<br>
Resolve to IPv4 addresses.
<br>
CURL_IPRESOLVE_V6
<br>
Resolve to IPv6 addresses.
<br>
</dd>
<dt>connect_only</dt><dd>Pass a long. If the parameter equals 1, it tells the library to perform all the required proxy authentication and connection setup, but no data transfer. This option is useful only on HTTP URLs.
<br>
This option is useful with the CURLINFO_LASTSOCKET option to curl_easy_getinfo(3). The library can set up the connection and then the application can obtain the most recently used socket for special data transfers. (Added in 7.15.2) </dd>
<dt>sslcert</dt><dd>Pass a pointer to a zero terminated string as parameter. The string should be the file name of your certificate. The default format is "PEM" and can be changed with <code>sslcerttype</code>.
<br>
With NSS this is the nickname of the certificate you wish to authenticate with.
</dd>
<dt>sslcerttype</dt><dd>Pass a pointer to a zero terminated string as parameter. The string should be the format of your certificate. Supported formats are "PEM" and "DER".  (Added in 7.9.3)
</dd>
<dt>sslkey</dt><dd>Pass a pointer to a zero terminated string as parameter. The string should be the file name of your private key. The default format is "PEM" and can be changed with <code>sslkeytype</code>.
</dd>
<dt>sslkeytype</dt><dd>Pass a pointer to a zero terminated string as parameter. The string should be the format of your private key. Supported formats are "PEM", "DER" and "ENG".
<br>
The format "ENG" enables you to load the private key from a crypto engine. In this case <code>sslkey</code> is used as an identifier passed to the engine. You have to set the crypto engine with <code>sslengine</code>. "DER" format key file currently does not work because of a bug in OpenSSL.
</dd>
<dt>keypasswd</dt><dd>Pass a pointer to a zero terminated string as parameter. It will be used as the password required to use the <code>sslkey</code> or <code>ssh_private_keyfile</code> private key. You never needed a pass phrase to load a certificate but you need one to load your private key.
<br>
(This option was known as <code>sslkeypasswd</code> up to 7.16.4 and <code>sslcertpasswd</code> up to 7.9.2)
</dd>
<dt>sslengine</dt><dd>Pass a pointer to a zero terminated string as parameter. It will be used as the identifier for the crypto engine you want to use for your private key.
<br>
If the crypto device cannot be loaded, CURLE_SSL_ENGINE_NOTFOUND is returned.
</dd>
<dt>sslengine_default</dt><dd>Sets the actual crypto engine as the default for (asymmetric) crypto operations.
<br>
If the crypto device cannot be set, CURLE_SSL_ENGINE_SETFAILED is returned.
<br>
Note that even though this option doesn't need any parameter, in some configurations curl_easy_setopt might be defined as a macro taking exactly three arguments. Therefore, it's recommended to pass 1 as parameter to this option.
</dd>
<dt>sslversion</dt><dd>Pass a long as parameter to control what version of SSL/TLS to attempt to use. The available options are:
<br>
CURL_SSLVERSION_DEFAULT
<br>
The default action. This will attempt to figure out the remote SSL protocol version, i.e. either SSLv3 or TLSv1 (but not SSLv2, which became disabled by default with 7.18.1).
<br>
CURL_SSLVERSION_TLSv1
<br>
Force TLSv1
<br>
CURL_SSLVERSION_SSLv2
<br>
Force SSLv2
<br>
CURL_SSLVERSION_SSLv3
<br>
Force SSLv3
<br>
</dd>
<dt>ssl_verifypeer</dt><dd>Pass a long as parameter.
<br>
This option determines whether curl verifies the authenticity of the peer's certificate. A value of 1 means curl verifies; zero means it doesn't.  The default is nonzero, but before 7.10, it was zero.
<br>
When negotiating an SSL connection, the server sends a certificate indicating its identity.  Curl verifies whether the certificate is authentic, i.e. that you can trust that the server is who the certificate says it is.  This trust is based on a chain of digital signatures, rooted in certification authority (CA) certificates you supply.  As of 7.10, curl installs a default bundle of CA certificates and you can specify alternate certificates with the <code>cainfo</code> option or the <code>capath</code> option.
<br>
When <code>ssl_verifypeer</code> is nonzero, and the verification fails to prove that the certificate is authentic, the connection fails.  When the option is zero, the connection succeeds regardless.
<br>
Authenticating the certificate is not by itself very useful.  You typically want to ensure that the server, as authentically identified by its certificate, is the server you mean to be talking to.  Use <code>ssl_verifyhost</code> to control that.
</dd>
<dt>cainfo</dt><dd>Pass a string to a zero terminated string naming a file holding one or more certificates to verify the peer with.  This makes sense only when used in combination with the <code>ssl_verifypeer</code> option.  If <code>ssl_verifypeer</code> is zero, <code>cainfo</code> need not even indicate an accessible file.
<br>
Note that option is by default set to the system path where libcurl's cacert bundle is assumed to be stored, as established at build time.
<br>
When built against NSS, this is the directory that the NSS certificate database resides in.
</dd>
<dt>issuercert</dt><dd>Pass a string to a zero terminated string naming a file holding a CA certificate in PEM format. If the option is set, an additional check against the peer certificate is performed to verify the issuer is indeed the one associated with the certificate provided by the option. This additional check is useful in multi-level PKI where one needs to enforce that the peer certificate is from a specific branch of the tree.
<br>
This option makes sense only when used in combination with the <code>ssl_verifypeer</code> option. Otherwise, the result of the check is not considered as failure.
<br>
A specific error code (CURLE_SSL_ISSUER_ERROR) is defined with the option, which is returned if the setup of the SSL/TLS session has failed due to a mismatch with the issuer of peer certificate (<code>ssl_verifypeer</code> has to be set too for the check to fail). (Added in 7.19.0)
</dd>
<dt>capath</dt><dd>Pass a string to a zero terminated string naming a directory holding multiple CA certificates to verify the peer with. The certificate directory must be prepared using the openssl c_rehash utility. This makes sense only when used in combination with the <code>ssl_verifypeer</code> option.  If <code>ssl_verifypeer</code> is zero, <code>capath</code> need not even indicate an accessible path.  The <code>capath</code> function apparently does not work in Windows due to some limitation in openssl. This option is OpenSSL-specific and does nothing if libcurl is built to use GnuTLS.
</dd>
<dt>crlfile</dt><dd>Pass a string to a zero terminated string naming a file with the concatenation of CRL (in PEM format) to use in the certificate validation that occurs during the SSL exchange.
<br>
When curl is built to use NSS or GnuTLS, there is no way to influence the use of CRL passed to help in the verification process. When libcurl is built with OpenSSL support, X509_V_FLAG_CRL_CHECK and X509_V_FLAG_CRL_CHECK_ALL are both set, requiring CRL check against all the elements of the certificate chain if a CRL file is passed.
<br>
This option makes sense only when used in combination with the <code>ssl_verifypeer</code> option.
<br>
A specific error code (CURLE_SSL_CRL_BADFILE) is defined with the option. It is returned when the SSL exchange fails because the CRL file cannot be loaded. Note that a failure in certificate verification due to a revocation information found in the CRL does not trigger this specific error. (Added in 7.19.0)
</dd>
<dt>certinfo</dt><dd>Pass a long set to 1 to enable libcurl's certificate chain info gatherer. With this enabled, libcurl (if built with OpenSSL) will extract lots of information and data about the certificates in the certificate chain used in the SSL connection. This data is then possible to extract after a transfer using curl_easy_getinfo(3) and its option CURLINFO_CERTINFO. (Added in 7.19.1)
</dd>
<dt>random_file</dt><dd>Pass a string to a zero terminated file name. The file will be used to read from to seed the random engine for SSL. The more random the specified file is, the more secure the SSL connection will become.
</dd>
<dt>egdsocket</dt><dd>Pass a string to the zero terminated path name to the Entropy Gathering Daemon socket. It will be used to seed the random engine for SSL.
</dd>
<dt>ssl_verifyhost</dt><dd>Pass a long as parameter.
<br>
This option determines whether libcurl verifies that the server cert is for the server it is known as.
<br>
When negotiating a SSL connection, the server sends a certificate indicating its identity.
<br>
When <code>ssl_verifyhost</code> is 2, that certificate must indicate that the server is the server to which you meant to connect, or the connection fails.
<br>
Curl considers the server the intended one when the Common Name field or a Subject Alternate Name field in the certificate matches the host name in the URL to which you told Curl to connect.
<br>
When the value is 1, the certificate must contain a Common Name field, but it doesn't matter what name it says.  (This is not ordinarily a useful setting).
<br>
When the value is 0, the connection succeeds regardless of the names in the certificate.
<br>
The default, since 7.10, is 2.
<br>
This option controls checking the server's claimed identity.  The server could be lying.  To control lying, see <code>ssl_verifypeer</code>.
</dd>
<dt>ssl_cipher_list</dt><dd>Pass a string, pointing to a zero terminated string holding the list of ciphers to use for the SSL connection. The list must be syntactically correct, it consists of one or more cipher strings separated by colons. Commas or spaces are also acceptable separators but colons are normally used, !, - and + can be used as operators.
<br>
For OpenSSL and GnuTLS valid examples of cipher lists include 'RC4-SHA', &lt;c2&gt;&lt;b4&gt;SHA1+DES&lt;c2&gt;&lt;b4&gt;, 'TLSv1' and 'DEFAULT'. The default list is normally set when you compile OpenSSL.
<br>
You'll find more details about cipher lists on this URL: http://www.openssl.org/docs/apps/ciphers.html
<br>
For NSS, valid examples of cipher lists include 'rsa_rc4_128_md5', &lt;c2&gt;&lt;b4&gt;rsa_aes_128_sha&lt;c2&gt;&lt;b4&gt;, etc. With NSS you don't add/remove ciphers. If one uses this option then all known ciphers are disabled and only those passed in are enabled.
<br>
You'll find more details about the NSS cipher lists on this URL: http://directory.fedora.redhat.com/docs/mod_nss.html#Directives
<br>
</dd>
<dt>ssl_sessionid_cache</dt><dd>Pass a long set to 0 to disable libcurl's use of SSL session-ID caching. Set this to 1 to enable it. By default all transfers are done using the cache. Note that while nothing ever should get hurt by attempting to reuse SSL session-IDs, there seem to be broken SSL implementations in the wild that may require you to disable this in order for you to succeed. (Added in 7.16.0)
</dd>
<dt>krblevel</dt><dd>Pass a string as parameter. Set the kerberos security level for FTP; this also enables kerberos awareness.  This is a string, 'clear', 'safe', 'confidential' or 'private'.  If the string is set but doesn't match one of these, 'private' will be used. Set the string to NULL to disable kerberos support for FTP.
<br>
(This option was known as <code>krb</code>4LEVEL up to 7.16.3) </dd>
<dt>ssh_auth_types</dt><dd>Pass a long set to a bitmask consisting of one or more of CURLSSH_AUTH_PUBLICKEY, CURLSSH_AUTH_PASSWORD, CURLSSH_AUTH_HOST, CURLSSH_AUTH_KEYBOARD. Set CURLSSH_AUTH_ANY to let libcurl pick one. (Added in 7.16.1)
</dd>
<dt>ssh_host_public_key_md5</dt><dd>Pass a string pointing to a string containing 32 hexadecimal digits. The string should be the 128 bit MD5 checksum of the remote host's public key, and libcurl will reject the connection to the host unless the md5sums match. This option is only for SCP and SFTP transfers. (Added in 7.17.1)
</dd>
<dt>ssh_public_keyfile</dt><dd>Pass a string pointing to a file name for your public key. If not used, libcurl defaults to using ~/.ssh/id_dsa.pub. (Added in 7.16.1)
</dd>
<dt>ssh_private_keyfile</dt><dd>Pass a string pointing to a file name for your private key. If not used, libcurl defaults to using ~/.ssh/id_dsa. If the file is password-protected, set the password with <code>keypasswd</code>. (Added in 7.16.1) </dd>
<dt>private</dt><dd>Pass a void * as parameter, pointing to data that should be associated with this curl handle.  The pointer can subsequently be retrieved using curl_easy_getinfo(3) with the CURLINFO_PRIVATE option. libcurl itself does nothing with this data. (Added in 7.10.3)
</dd>
<dt>share</dt><dd>Pass a share handle as a parameter. The share handle must have been created by a previous call to curl_share_init(3). Setting this option, will make this curl handle use the data from the shared handle instead of keeping the data to itself. This enables several curl handles to share data. If the curl handles are used simultaneously in multiple threads, you MUST use the locking methods in the share handle. See curl_share_setopt(3) for details.
<br>
If you add a share that is set to share cookies, your easy handle will use that cookie cache and get the cookie engine enabled. If you unshare an object that was using cookies (or change to another object that doesn't share cookies), the easy handle will get its cookie engine disabled.
<br>
Data that the share object is not set to share will be dealt with the usual way, as if no share was used.
</dd>
<dt>new_file_perms</dt><dd>Pass a long as a parameter, containing the value of the permissions that will be assigned to newly created files on the remote server.  The default value is 0644, but any valid value can be used.  The only protocols that can use this are sftp://, scp://, and file://. (Added in 7.16.4)
</dd>
<dt>new_directory_perms</dt><dd>Pass a long as a parameter, containing the value of the permissions that will be assigned to newly created directories on the remote server.  The default value is 0755, but any valid value can be used.  The only protocols that can use this are sftp://, scp://, and file://. (Added in 7.16.4) </dd>
<dt>telnetoptions</dt><dd>Provide a pointer to a curl_slist with variables to pass to the telnet negotiations. The variables should be in the format &lt;option=value&gt;. libcurl supports the options 'TTYPE', 'XDISPLOC' and 'NEW_ENV'. See the TELNET standard for details. </dd>
</ul>
</td></tr>
<tr valign="top"><td><code>.opts</code></td>
<td>
a named list of options, typically a previously created
<code>CURLOptions</code> object. These are merged with the options
specified in <code>...</code>.</td></tr>
<tr valign="top"><td><code>x</code></td>
<td>
a <code>CURLOptions</code> object</td></tr>
<tr valign="top"><td><code>i</code></td>
<td>
the name(s) of the option elements being accessed.
These can be partial names matching elements in the set
of known options. Other names will cause an error.</td></tr>
<tr valign="top"><td><code>value</code></td>
<td>
the values to assign to the options identified via <code>i</code>.</td></tr>
</table>

<h3>Details</h3>

<p>
These functions use <code>mapCurlOptNames</code>
to match and hence expand the names the callers
provide.
</p>


<h3>Value</h3>

<p>
<code>curlOptions</code> returns an object
of class <code>CURLOptions</code> which is simply
a named list.
<br>
<code>getCurlConstants</code> returns a named vector identifying
the names of the possible options and their associated
values. These values are used in the C code and also each integer
encodes the type of the argument expected by the C code
for that option.</p>

<h3>Author(s)</h3>

<p>
Duncan Temple Lang &lt;duncan@wald.ucdavis.edu&gt;
</p>


<h3>References</h3>

<p>
Curl homepage <a href="http://curl.haxx.se">http://curl.haxx.se</a>
</p>


<h3>See Also</h3>

<p>
<code><a href="../../../doc/html/search/SearchObject.html?curlPerform">curlPerform</a></code>
<code><a href="../../../doc/html/search/SearchObject.html?curlSetOpt">curlSetOpt</a></code>
</p>


<h3>Examples</h3>

<pre>

 tt = basicTextGatherer()
 myOpts = curlOptions(verbose = TRUE, header = TRUE, writefunc = tt[[1]])

  # note that the names are expanded, e.g. writefunc is now writefunction.
 names(myOpts)

 myOpts[["header"]]

 myOpts[["header"]] &lt;- FALSE

# Using the abbreviation "hea" is an error as it matches
# both 
#  myOpts[["hea"]] &lt;- FALSE

 # Remove the option from the list
 myOpts[["header"]] &lt;- NULL
</pre>



<hr><div align="center">[Package <a href="00Index.html">Index]</a></div>

</body></html>
